{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.reflacx import REFLACXDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# import pandas as pd\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = REFLACXDataset(\n",
    "    df_path=\"./spreadsheets/reflacx_clinical.csv\",\n",
    "    mimic_eye_path=\"F:\\\\mimic-eye\",\n",
    "    image_size=128,\n",
    "    split_str=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    d,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn= lambda x: tuple(zip(*x)),\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mike8\\OneDrive\\文件\\GitHub\\fusion-experiments\\dataset\\reflacx.py:205: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(data[self.chexpert_label_cols]) == 1\n",
      "c:\\Users\\mike8\\OneDrive\\文件\\GitHub\\fusion-experiments\\dataset\\reflacx.py:223: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(data[self.clinical_label])\n",
      "c:\\Users\\mike8\\OneDrive\\文件\\GitHub\\fusion-experiments\\dataset\\reflacx.py:205: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(data[self.chexpert_label_cols]) == 1\n",
      "c:\\Users\\mike8\\OneDrive\\文件\\GitHub\\fusion-experiments\\dataset\\reflacx.py:223: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(data[self.clinical_label])\n",
      "c:\\Users\\mike8\\OneDrive\\文件\\GitHub\\fusion-experiments\\dataset\\reflacx.py:205: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(data[self.chexpert_label_cols]) == 1\n",
      "c:\\Users\\mike8\\OneDrive\\文件\\GitHub\\fusion-experiments\\dataset\\reflacx.py:223: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(data[self.clinical_label])\n",
      "c:\\Users\\mike8\\OneDrive\\文件\\GitHub\\fusion-experiments\\dataset\\reflacx.py:205: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(data[self.chexpert_label_cols]) == 1\n",
      "c:\\Users\\mike8\\OneDrive\\文件\\GitHub\\fusion-experiments\\dataset\\reflacx.py:223: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(data[self.clinical_label])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['xray', 'bb_label', 'clinical_data', 'chexpert_label', 'report'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = ['xray', 'bb_label', 'clinical_data', 'chexpert_label', 'report']\n",
    "\n",
    "batch_map = {\n",
    "    m: [ d[m] for d in data[0]]\n",
    "    for m in modalities\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack(batch_map['xray'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 128, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1 # batch_size\n",
    "n, c, h, w = imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "patch_size = 8\n",
    "\n",
    "conv_proj = nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=hidden_dim,\n",
    "    kernel_size=patch_size,\n",
    "    stride=patch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h = h // patch_size\n",
    "n_w = w // patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_seq  = conv_proj(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_inputs = img_seq.reshape(n, hidden_dim, n_h*n_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5405], dtype=torch.float64),\n",
       " tensor([0.6974], dtype=torch.float64),\n",
       " tensor([-0.8318], dtype=torch.float64),\n",
       " tensor([0.1149], dtype=torch.float64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_label = batch_map['bb_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_label_emb_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_embedding_layer = nn.Embedding(\n",
    "    num_embeddings=len(loader.dataset.label_cols)+1,\n",
    "    embedding_dim=bb_label_emb_dim,\n",
    "    padding_idx=0,\n",
    ")\n",
    "bb_fc = nn.Linear(4+bb_label_emb_dim, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_inputs = []\n",
    "for d in bb_label:\n",
    "    if len(d[\"boxes\"]) > 0:\n",
    "        emb_l = bb_embedding_layer(d[\"labels\"])\n",
    "        bbs = bb_fc(torch.concat([d[\"boxes\"], emb_l], dim=1))\n",
    "        bb_inputs.append(bbs)\n",
    "    else:\n",
    "        bb_inputs.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_cat_emb = nn.Embedding(2,  32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical = batch_map['clinical_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cat': {'gender': tensor(1, dtype=torch.int32)}, 'num': tensor([-0.4286])},\n",
       " {'cat': {'gender': tensor(0, dtype=torch.int32)}, 'num': tensor([0.7073])},\n",
       " {'cat': {'gender': tensor(0, dtype=torch.int32)}, 'num': tensor([-0.2123])},\n",
       " {'cat': {'gender': tensor(1, dtype=torch.int32)}, 'num': tensor([-0.7532])}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_map = {\n",
    "    \"cat\": {},\n",
    "}\n",
    "for c in clinical:\n",
    "    for k in c[\"cat\"].keys():\n",
    "        if not k in clinical_map[\"cat\"]:\n",
    "            clinical_map[\"cat\"][k] = []\n",
    "        clinical_map[\"cat\"][k].append(c[\"cat\"][k])\n",
    "\n",
    "clinical_map[\"num\"] = [c[\"num\"] for c in clinical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_cat_emb = {\n",
    "    \"gender\": nn.Embedding(2, 32),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_cat = clinical_cat_emb['gender'](torch.stack(clinical_map[\"cat\"][\"gender\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_num = torch.stack(clinical_map['num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_fc = nn.Linear(33, 128)\n",
    "clinical_inputs = clinical_fc(torch.concat([clinical_cat,clinical_num],dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_inputs = clinical_inputs.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chesxpert_label = batch_map['chexpert_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_fc = nn.Linear(14, 128)\n",
    "chexpert_inputs= chexpert_fc(torch.stack(chesxpert_label).float()).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chexpert_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import BioGptTokenizer\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "token_emb = TokenEmbedding(tokenizer.vocab_size, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    batch_map['report'],\n",
    "    # return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_inputs = [token_emb(torch.tensor(n)) for n in inputs['input_ids']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102, 128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([111, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_inputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " tensor([[ 5.9379e-01, -5.3448e-01, -6.2671e-01, -4.8388e-01, -6.7895e-01,\n",
       "           1.4315e-01, -1.5155e+00,  6.2987e-01, -9.9504e-03, -1.5155e-01,\n",
       "          -9.1749e-03,  1.7801e-01,  1.4580e-01, -7.1542e-01,  1.5403e-02,\n",
       "          -1.9494e-01,  4.9911e-01, -7.2490e-02,  3.1341e-01,  1.0999e+00,\n",
       "          -9.6446e-02, -4.6743e-01, -8.9039e-01, -4.0554e-01,  3.6663e-01,\n",
       "           2.1572e-01, -1.1009e-02, -3.7145e-01, -2.8702e-01, -3.4386e-01,\n",
       "          -4.9316e-01,  4.3988e-01,  3.0049e-01,  3.8851e-01,  1.7379e-01,\n",
       "           1.2000e-01, -6.3587e-01,  3.5331e-01,  5.3285e-01, -2.7295e-01,\n",
       "           1.5249e-01,  9.3246e-01, -4.1730e-01,  5.9662e-01,  8.3072e-01,\n",
       "           3.3380e-01, -9.6957e-01,  3.3031e-01, -1.1813e-01,  1.4225e-01,\n",
       "           7.4860e-01,  3.9765e-03,  1.5640e-01, -4.9435e-01, -4.1868e-01,\n",
       "          -6.5998e-01, -7.2326e-02,  2.5672e-02, -1.0318e+00,  3.5936e-01,\n",
       "          -3.1958e-01,  1.3043e-03,  1.0909e-01,  2.9044e-01,  7.6512e-02,\n",
       "           5.8948e-01,  4.0331e-01,  3.0517e-01, -6.1641e-01,  2.3727e-01,\n",
       "           1.0148e+00,  1.5353e-01, -4.8329e-01, -4.6895e-02,  6.1540e-01,\n",
       "          -3.9577e-01,  1.0756e-01, -4.3241e-01, -3.1664e-01,  1.8104e-01,\n",
       "          -3.9157e-01, -1.8908e-01,  6.8202e-01, -4.8293e-01,  3.2575e-01,\n",
       "          -1.9618e-01,  2.7995e-01, -8.3323e-02, -2.6691e-01, -1.4305e-01,\n",
       "           6.7934e-01,  5.4998e-01,  1.4168e-01, -1.1856e+00,  2.1417e-01,\n",
       "          -1.1427e+00,  5.2763e-02, -7.4823e-01,  1.2513e-01,  8.7536e-02,\n",
       "          -2.2450e-01, -3.9614e-01,  6.7287e-01, -2.0598e-01, -4.5657e-01,\n",
       "           8.0435e-01,  1.4920e-01,  1.0279e+00, -4.7665e-01,  7.3094e-01,\n",
       "          -4.1566e-01,  1.3932e+00,  1.8908e+00,  1.6388e-01,  4.5624e-01,\n",
       "          -1.9594e-01, -2.3811e-01,  1.1579e-01,  2.8495e-01,  5.1853e-01,\n",
       "          -9.7935e-01, -2.0254e-01,  6.9104e-01,  5.6527e-01, -8.7993e-01,\n",
       "           1.1248e-01, -4.6873e-01, -1.8600e-01]], grad_fn=<AddmmBackward0>)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chexpert_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 13.9371,  -1.0265,   3.0123,  ...,   4.0277,   1.7066, -20.8931],\n",
       "         [  4.7147,  -3.2481, -14.8378,  ...,  -0.1010,  -7.4070,   2.5657],\n",
       "         [ -0.7778,   6.3151,   1.2750,  ...,   4.0703, -10.1901, -14.5043],\n",
       "         ...,\n",
       "         [  1.5462,  10.8573,   6.5433,  ...,  -1.2977, -12.6543,   5.8778],\n",
       "         [ 15.1477, -17.3649,  -7.9275,  ...,  17.5019,  -6.6169,   4.3734],\n",
       "         [ 25.8858,   4.3370, -10.0249,  ...,   9.7746,   8.9536,  10.3916]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[ 13.9371,  -1.0265,   3.0123,  ...,   4.0277,   1.7066, -20.8931],\n",
       "         [  4.7147,  -3.2481, -14.8378,  ...,  -0.1010,  -7.4070,   2.5657],\n",
       "         [ -0.7778,   6.3151,   1.2750,  ...,   4.0703, -10.1901, -14.5043],\n",
       "         ...,\n",
       "         [ -5.5855,  -4.6550,  -7.6614,  ...,   9.6503, -11.5157,   9.9651],\n",
       "         [-19.5617,   1.5089, -10.5035,  ...,   8.4118, -14.7911,   3.0680],\n",
       "         [ 25.8858,   4.3370, -10.0249,  ...,   9.7746,   8.9536,  10.3916]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[ 1.3937e+01, -1.0265e+00,  3.0123e+00,  ...,  4.0277e+00,\n",
       "           1.7066e+00, -2.0893e+01],\n",
       "         [ 3.6914e+00, -1.5370e+01,  8.6934e+00,  ..., -2.1863e+00,\n",
       "          -4.2455e+00,  2.9243e+01],\n",
       "         [-4.3726e+00,  4.1844e+00,  6.0179e+00,  ..., -7.8707e+00,\n",
       "           4.9266e+00,  6.3027e+00],\n",
       "         ...,\n",
       "         [ 9.8736e+00,  1.9716e-01, -6.9469e+00,  ...,  9.6139e+00,\n",
       "           1.7164e+01, -2.3059e+00],\n",
       "         [ 1.3669e+01,  2.9748e+00, -6.3763e+00,  ...,  3.3034e+00,\n",
       "           9.6738e+00,  2.6430e-02],\n",
       "         [ 2.5886e+01,  4.3370e+00, -1.0025e+01,  ...,  9.7746e+00,\n",
       "           8.9536e+00,  1.0392e+01]], grad_fn=<MulBackward0>),\n",
       " tensor([[ 13.9371,  -1.0265,   3.0123,  ...,   4.0277,   1.7066, -20.8931],\n",
       "         [  8.0118, -17.3483,  -3.6743,  ...,  10.2461,  -7.1993,  -1.9366],\n",
       "         [ 29.0340,  -2.4167,   6.2831,  ...,  -2.4702,  -4.2914,  10.1111],\n",
       "         ...,\n",
       "         [ -5.8504,  -9.2485,  -1.5838,  ...,  -0.3094,  16.3066,  32.7908],\n",
       "         [ 18.3661,   9.5892,  -1.2005,  ...,  15.2651,   0.5968,   7.1966],\n",
       "         [ 25.8858,   4.3370, -10.0249,  ...,   9.7746,   8.9536,  10.3916]],\n",
       "        grad_fn=<MulBackward0>)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_tensor = token_emb(torch.tensor([tokenizer.bos_token_id])).reshape(-1, 1)\n",
    "eos_tensor = token_emb(torch.tensor([tokenizer.eos_token_id])).reshape(-1, 1)\n",
    "pad_tensor = token_emb(torch.tensor([tokenizer.pad_token_id])).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = []\n",
    "for img, bb, c, chexpert, report in zip(\n",
    "    img_inputs, bb_inputs, clinical_inputs, chexpert_inputs, report_inputs\n",
    "):\n",
    "    cat_list = [\n",
    "        bos_tensor,\n",
    "        img,\n",
    "        eos_tensor,\n",
    "        bos_tensor,\n",
    "        c.reshape(hidden_dim, -1),\n",
    "        eos_tensor,\n",
    "        bos_tensor,\n",
    "        chexpert.reshape(hidden_dim, -1),\n",
    "        eos_tensor,\n",
    "        bos_tensor,\n",
    "        report.permute(1, 0),\n",
    "        eos_tensor,\n",
    "    ]\n",
    "\n",
    "    if not bb is None and len(bb) > 0:\n",
    "        cat_list += [\n",
    "            bos_tensor,\n",
    "            bb.permute(1, 0),\n",
    "            eos_tensor,\n",
    "        ]\n",
    "\n",
    "    cat_t = torch.concat(cat_list, dim=1)\n",
    "    input_list.append(cat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = []\n",
    "\n",
    "max_len = max([ i.shape[-1] for i in input_list])\n",
    "for i in input_list:\n",
    "    if i.shape[-1] < max_len:\n",
    "        padded_inputs.append(torch.concat([i, pad_tensor.repeat(1, max_len-i.shape[-1])], dim=1))\n",
    "    else:\n",
    "        padded_inputs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_inputs = torch.stack(padded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_inputs = cat_inputs.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 382, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.TransformerEncoderLayer(\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    batch_first=True,\n",
    ")\n",
    "model = nn.TransformerEncoder(layer, num_layers=4, enable_nested_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = PositionalEncoding(hidden_dim, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(torch.nested.nested_tensor(input_list)) # inference only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pos_enc(cat_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0]*50+[1]*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the last one and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:, -1 , :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_proj =nn.Linear(hidden_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = output_proj(outputs[:, -1 , :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "torch.rand((3,3))[:, a,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx= torch.tensor([1, 2,1]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, idx, torch.arange(a.shape[-1])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(a, 1, torch.tensor([1,2])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3337],\n",
       "        [ 0.1281],\n",
       "        [ 0.3768],\n",
       "        [-0.8121]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5518],\n",
       "        [-0.3220],\n",
       "        [-0.5405],\n",
       "        [-0.1764]], dtype=torch.float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2733, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(out, torch.stack(data[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
